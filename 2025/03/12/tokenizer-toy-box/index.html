<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Xin">





<title>Tokenizer Playbook: BPE, SentencePiece, and Multilingual Tricks | Xin&#39;s Blog</title>



    <link rel="icon" href="/image/avatar.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const pagebody = document.getElementsByTagName('body')[0]

            function setTheme(status) {

                if (status === 'dark') {
                    window.sessionStorage.theme = 'dark'
                    pagebody.classList.add('dark-theme');

                } else if (status === 'light') {
                    window.sessionStorage.theme = 'light'
                    pagebody.classList.remove('dark-theme');
                }
            };

            setTheme(window.sessionStorage.theme)
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">知りたいままに | Xin&#39;s Notebook</a></div>
            <div class="menu navbar-right">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/files/resume_0223.pdf">Resume</a>
                
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">知りたいままに | Xin&#39;s Notebook</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">
                    <svg class="menu-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M4.5 17.27q-.213 0-.356-.145T4 16.768t.144-.356t.356-.143h15q.213 0 .356.144q.144.144.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.144T4 11.999t.144-.356t.356-.143h15q.213 0 .356.144t.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.143Q4 7.443 4 7.23t.144-.356t.356-.143h15q.213 0 .356.144T20 7.23t-.144.356t-.356.144z"/></svg>
                    <svg class="close-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Material Symbols Light by Google - https://github.com/google/material-design-icons/blob/master/LICENSE --><path fill="currentColor" d="m12 12.708l-5.246 5.246q-.14.14-.344.15t-.364-.15t-.16-.354t.16-.354L11.292 12L6.046 6.754q-.14-.14-.15-.344t.15-.364t.354-.16t.354.16L12 11.292l5.246-5.246q.14-.14.345-.15q.203-.01.363.15t.16.354t-.16.354L12.708 12l5.246 5.246q.14.14.15.345q.01.203-.15.363t-.354.16t-.354-.16z"/></svg>
                </div>
            </div>
            <div class="menu" id="mobile-menu">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/files/resume_0223.pdf">Resume</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if (toggleMenu.classList.contains("active")) {
            toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        } else {
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Tokenizer Playbook: BPE, SentencePiece, and Multilingual Tricks</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Xin</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">March 12, 2025&nbsp;&nbsp;15:09:21</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>This post is part of my journey toward pre-training. I see a near future where AI shifts from monolithic “AGI” toward orchestrated <strong>ASI (Artificial Specialized Intelligence)</strong> systems. If Mixture-of-Experts (MoE) is hard to train and expensive to serve, why not split the giant model into several smaller models that are easier to verify, control, deploy, and even run locally?</p>
<p>For that, I’m starting with a core building block: <strong>tokenizers</strong>.</p>
<h2 id="Basics">Basics</h2>
<p>A <strong>tokenizer</strong> maps between text and integers.</p>
<ul>
<li><strong>Encode</strong>: take a string → output a list of integers (tokens)</li>
<li><strong>Decode</strong>: take a list of integers → reconstruct the string</li>
</ul>
<p>Why do we need this? Because training and inference are linear-algebra on numbers, not raw strings.</p>
<p><strong>Libraries</strong></p>
<ul>
<li>Use <strong><code>tiktoken</code></strong> when you need compatibility with OpenAI-style vocabularies.</li>
<li>Use <strong>SentencePiece</strong> when you want to <strong>train your own tokenizer</strong> (custom domains, multilingual corpora, pre-training from scratch).</li>
</ul>
<h2 id="Tokenization-Compression">Tokenization &amp; Compression</h2>
<p>Modern tokenization borrows ideas from <strong>data compression</strong>.</p>
<p><strong>Step 1 — Represent text as integers.</strong><br>
The simplest approach is character IDs:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;hi yo&quot; -&gt; [&#x27;h&#x27;,&#x27;i&#x27;,&#x27; &#x27;,&#x27;y&#x27;,&#x27;o&#x27;] -&gt; [46, 47, 1, 63, 53]</span><br></pre></td></tr></table></figure>
<p>This works but offers <strong>no compression</strong> — longer sequences mean higher cost, slower speed, and shorter effective context.</p>
<p><strong>Step 2 — Compress.</strong><br>
Methods like Byte Pair Encoding (BPE) iteratively merge the most frequent adjacent pairs into larger subword units, shrinking average sequence length while retaining full coverage.</p>
<blockquote>
<p>Quick mental model: frequent words/subwords become single tokens; rare words fall back to smaller pieces.</p>
</blockquote>
<h2 id="Out-of-Vocabulary-OOV">Out of Vocabulary (OOV)</h2>
<p>Modern subword tokenizers avoid “true” OOV. Any string can be expressed as subwords (or ultimately characters/bytes). The trade-off is longer sequences for rare or unusual strings.</p>
<h2 id="Training-or-Adapting-a-Tokenizer-for-Pre-Training">Training (or Adapting) a Tokenizer for Pre-Training</h2>
<p>If you’re training a specialized model with rare strings—e.g., clinical dialogs with ICD-10 codes—and you want codes like E11.9 to stay intact, you have two main paths:</p>
<h3 id="Use-an-existing-tokenizer-and-add-special-tokens">Use an existing tokenizer and add special tokens</h3>
<p>Low effort and often sufficient.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"></span><br><span class="line">enc = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line">custom_enc = tiktoken.Encoding(</span><br><span class="line">    name=<span class="string">&quot;custom&quot;</span>,</span><br><span class="line">    pat_str=enc._pat_str,</span><br><span class="line">    mergeable_ranks=enc._mergeable_ranks,</span><br><span class="line">    special_tokens=&#123;**enc._special_tokens, <span class="string">&quot;E11.9&quot;</span>: enc.n_vocab&#125;  <span class="comment"># choose an unused ID</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokens = custom_enc.encode(<span class="string">&quot;Patient diagnosed with E11.9&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokens)  <span class="comment"># &quot;E11.9&quot; becomes a single token</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: This custom mapping affects your tokenizer. A model must have embeddings for these IDs (i.e., be trained/fine-tuned with them) to benefit.</p>
</blockquote>
<h3 id="Train-a-new-tokenizer-from-your-dataset">Train a new tokenizer from your dataset</h3>
<p>Most flexible, especially for <strong>multilingual data</strong> (English, Chinese, Japanese, Russian, French, etc.). A strong default is <strong>a joint multilingual Unigram model</strong> with SentencePiece.</p>
<h4 id="Training-Recipe-SentencePiece">Training Recipe (SentencePiece)</h4>
<p>Guidelines</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Model: unigram</span><br><span class="line">Vocab size: 64k–80k (increase if your corpus has lots of code/emoji/URLs)</span><br><span class="line">Normalization: Unicode NFKC (default); do not lowercase if case matters (ICD codes)</span><br><span class="line">CJK coverage: --character_coverage=0.9995 for mixed CJK + Latin</span><br><span class="line">Robustness: --byte_fallback=true (ensures no true OOV)</span><br><span class="line">Domain symbols: pass a user_symbols.txt (e.g., common ICD-10 codes) so they become indivisible tokens</span><br></pre></td></tr></table></figure>
<p>CLI</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spm_train \</span><br><span class="line">  --input=all_corpus.txt \</span><br><span class="line">  --model_prefix=spm_unigram_80k \</span><br><span class="line">  --model_type=unigram \</span><br><span class="line">  --vocab_size=80000 \</span><br><span class="line">  --character_coverage=0.9995 \</span><br><span class="line">  --byte_fallback=<span class="literal">true</span> \</span><br><span class="line">  --user_defined_symbols=user_symbols.txt \</span><br><span class="line">  --unk_id=0 --bos_id=1 --eos_id=2 --pad_id=3 \</span><br><span class="line">  --input_sentence_size=5000000 --shuffle_input_sentence=<span class="literal">true</span> \</span><br><span class="line">  --hard_vocab_limit=<span class="literal">false</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Tip: Build user_symbols.txt from the top N domain strings in your corpus (e.g., most frequent ICD-10 codes). This preserves what matters without exploding the vocab.</p>
</blockquote>
<h4 id="Verifying-the-Tokenizer">Verifying the Tokenizer</h4>
<p>Evaluate on a held-out multilingual set:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tokens per char/word by language (EN/zh/ja/ru/fr). Aim for low token inflation, especially for CJK.</span><br><span class="line">ICD-10 stability: top codes should be single tokens; rare ones should still be short (e.g., E11 + .9).</span><br><span class="line">Round-trip fidelity: exact detokenization (keep punctuation like the dot in E11.9).</span><br><span class="line">Throughput &amp; memory: larger vocab → larger embeddings; find the sweet spot.</span><br></pre></td></tr></table></figure>
<p>A tiny probe:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line">sp = spm.SentencePieceProcessor(model_file=<span class="string">&quot;spm_unigram_80k.model&quot;</span>)</span><br><span class="line"></span><br><span class="line">samples = [</span><br><span class="line">  <span class="string">&quot;Dx: E11.9 Type 2 diabetes mellitus without complications.&quot;</span>,</span><br><span class="line">  <span class="string">&quot;患者被诊断为E11.9，并建议随访。&quot;</span>,</span><br><span class="line">  <span class="string">&quot;患者はE11.9と診断された。&quot;</span>,</span><br><span class="line">  <span class="string">&quot;Пациент с диагнозом E11.9.&quot;</span>,</span><br><span class="line">  <span class="string">&quot;Le patient présente un E11.9.&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> samples:</span><br><span class="line">    pieces = sp.encode(s, out_type=<span class="built_in">str</span>)</span><br><span class="line">    <span class="built_in">print</span>(s, <span class="string">&quot;\n -&gt;&quot;</span>, pieces, <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Notes-Alternatives">Notes &amp; Alternatives</h2>
<ul>
<li>Byte-level BPE (HF tokenizers) is excellent for mixed ASCII/code/emoji/URLs. For heavy CJK, Unigram often yields fewer tokens unless you tune BPE carefully.</li>
<li>If the universe of codes is huge and evolving, consider lightweight wrappers like <dx>E11.9</dx> plus a smaller set of pinned codes.</li>
<li>Up-weight domain lines when training the tokenizer so frequency-driven merges favor your symbols.</li>
</ul>
<p>Happy coding, good luck of modeling &lt;3</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Xin</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://arriettyc.github.io/2025/03/12/tokenizer-toy-box/">https://arriettyc.github.io/2025/03/12/tokenizer-toy-box/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2025/02/02/grpo_r1/">Deepseek GRPO and R1</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Xin | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>