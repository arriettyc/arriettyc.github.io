<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Xin">





<title>Deepseek GRPO and R1 | Xin&#39;s Blog</title>



    <link rel="icon" href="/image/avatar.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const pagebody = document.getElementsByTagName('body')[0]

            function setTheme(status) {

                if (status === 'dark') {
                    window.sessionStorage.theme = 'dark'
                    pagebody.classList.add('dark-theme');

                } else if (status === 'light') {
                    window.sessionStorage.theme = 'light'
                    pagebody.classList.remove('dark-theme');
                }
            };

            setTheme(window.sessionStorage.theme)
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">知りたいままに | Xin&#39;s Notebook</a></div>
            <div class="menu navbar-right">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/about">About</a>
                
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">知りたいままに | Xin&#39;s Notebook</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">
                    <svg class="menu-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M4.5 17.27q-.213 0-.356-.145T4 16.768t.144-.356t.356-.143h15q.213 0 .356.144q.144.144.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.144T4 11.999t.144-.356t.356-.143h15q.213 0 .356.144t.144.357t-.144.356t-.356.143zm0-4.77q-.213 0-.356-.143Q4 7.443 4 7.23t.144-.356t.356-.143h15q.213 0 .356.144T20 7.23t-.144.356t-.356.144z"/></svg>
                    <svg class="close-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Material Symbols Light by Google - https://github.com/google/material-design-icons/blob/master/LICENSE --><path fill="currentColor" d="m12 12.708l-5.246 5.246q-.14.14-.344.15t-.364-.15t-.16-.354t.16-.354L11.292 12L6.046 6.754q-.14-.14-.15-.344t.15-.364t.354-.16t.354.16L12 11.292l5.246-5.246q.14-.14.345-.15q.203-.01.363.15t.16.354t-.16.354L12.708 12l5.246 5.246q.14.14.15.345q.01.203-.15.363t-.354.16t-.354-.16z"/></svg>
                </div>
            </div>
            <div class="menu" id="mobile-menu">
                
                <a class="menu-item" href="/archives">Posts</a>
                
                <a class="menu-item" href="/category">Categories</a>
                
                <a class="menu-item" href="/tag">Tags</a>
                
                <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if (toggleMenu.classList.contains("active")) {
            toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        } else {
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Deepseek GRPO and R1</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Xin</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">February 2, 2025&nbsp;&nbsp;13:23:34</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>Across most popular open-source (OSS) models, there’s strong enthusiasm for climbing benchmark leaderboards. However, the majority of these benchmarks focus on pre-training performance — covering both reasoning and non-reasoning tasks.</p>
<p>In comparison, there has been relatively smaller attention given to OSS post-training, where most work has focused narrowly on improving instruction-following behavior.</p>
<p>That dynamic started to shift with the release of R1, which demonstrated the significant potential of large-scale, RL-driven post-training to meaningfully improve model capabilities. One line from the R1 technical report particularly stood out to me:</p>
<blockquote>
<p>“<em><strong>Post-training</strong> can be a powerful lever for <strong>boosting base model performance</strong> while keeping <strong>costs low</strong>.</em>“</p>
</blockquote>
<p>Since I’ll be giving a talk to the team on R1 and GRPO, I wanted to capture a few quick reflections based on what I’ve learned so far.</p>
<h2 id="Group-Relative-Policy-Optimization-GRPO"><a href="#Group-Relative-Policy-Optimization-GRPO" class="headerlink" title="Group Relative Policy Optimization (GRPO)"></a>Group Relative Policy Optimization (GRPO)</h2><p>Reward modeling is a core component of reinforcement learning (RL), as it defines the optimization direction of the entire process. Based on empirical experience, there are two fundamental principles for designing an effective reward model:</p>
<ol>
<li><p>Accurately capture your true optimization objective</p>
</li>
<li><p>Ensure consistency in reward signals</p>
</li>
</ol>
<p>The first principle can already be challenging — your reward model should provide a strong positive signal for preferred outputs (the “winners”) and a weaker or negative signal for undesirable ones (the “losers”). You can think of this like designing the perspective of an impartial judge or having a “god view” over the task.</p>
<p>The second principle — consistency — becomes even more difficult, especially for non-reasoning tasks. Take conversational AI as an example: some users prefer short, concise answers, while others expect more detailed, descriptive responses. In such cases, the reward signal can become subjective and even contradictory, depending on the evaluator’s preference. This inherent bias makes maintaining consistent reward signals particularly tricky in practice.</p>
<p>R1 series training adopts rule-based reward modeling, from it’s paper, two reward training signals,</p>
<ul>
<li>Accuracy reward, for math&#x2F;coding training data</li>
<li>Format rewards, enforce model to put thinking process between think token &lt;think&gt; .. &lt;&#x2F;think&gt;</li>
</ul>
<p>with this, defined a group based advantage as</p>
<p>$$A_i &#x3D; (r_i - mean({r_1, r_2, …, r_G})) &#x2F; std({r_1, r_2, …, r_G})$$</p>
<p>and plugged in training objective</p>
<p>$$J_{GRPO}(\theta) &#x3D; \mathbb{E}[q \sim P(Q), {o_i}<em>{i&#x3D;1}^G \sim \pi</em>{\theta_{old}}(O|q)] \left[ \frac{1}{G} \sum_{i&#x3D;1}^{G} \left( \min \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip} \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta \mathbb{D}<em>{KL}(\pi_\theta || \pi</em>{ref}) \right) \right]$$</p>
<p>where $r_i$ is the reward of output $o_i$. Intuitively, the GRPO update is comparing multiple answers to a single question within a batch. The model learns to become more like the answer marked as correct and less like the others. This is a very simple way to compare the advantage, which is the measure of how much better a specific action is than the average at a given state. Compared to PPO or generic RLs, GRPO can run wth a far higher number of samples per question&#x2F;prompt.</p>
<p><img src="/../../image/r1/ppo_grpo.png" alt="ppo_grpo"><br><em>Diagram Ref <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.03300">https://arxiv.org/pdf/2402.03300</a></em></p>
<p>Compared with PPO, it avoids learning value model which significant improved training efficiency and maintained a relatively clear&#x2F;deterministic learning signal, specific</p>
<ol>
<li>Training a good value model is challenging, where research hasn’t established best practice</li>
<li>Reduced memory and compute resource due to remove value model</li>
</ol>
<h2 id="Mandating-Reflective-Reasoning"><a href="#Mandating-Reflective-Reasoning" class="headerlink" title="Mandating Reflective Reasoning"></a>Mandating Reflective Reasoning</h2><p>For each question, requiring &#x2F; enforcing a process where a model explicitly works through its thought process or reasoning steps before arriving at the final answer</p>
<p><img src="/../../image/r1/think_prompt.png" alt="think_token"><br><em>Ref:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.12948">https://arxiv.org/pdf/2501.12948</a></em></p>
<p>intuitively, with this think token, the completion of the model is trending longer along training steps. Behaviors such as <strong>reflection</strong> – where the model revisits the reevaluates its previous steps – and the exploration of alternative approaches to problem-solving arise spontaneously. <strong>These behaviors are not explicitly programmed but instead emerge as a result of the model&#39;s interaction with the reinforcement learning environment</strong>. </p>
<p><img src="/../../image/r1/r1_perf_png.png" alt="perf"><br><em>Ref:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.12948">https://arxiv.org/pdf/2501.12948</a></em></p>
<p>This spontaneous development significantly enhances Deepseek-R1-Zero’s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.</p>
<h2 id="Deepseek-R1-RL-with-Cold-Start"><a href="#Deepseek-R1-RL-with-Cold-Start" class="headerlink" title="Deepseek-R1: RL with Cold Start"></a>Deepseek-R1: RL with Cold Start</h2><p>Objective: enhance reasoning performance and readability by incorporating a small amount of high-quality <strong>cold-start data</strong></p>
<img src="../../image/r1/r1_zero_vs_r1.png" alt="r1_zero_vs_r1" width="70%">

<p>I summarized this R1 workflow to demonstrate the training pipeline, blue rectangle stands for model checkpoint, the cylinder means dataset.</p>
<p>SFT (cold start data) -&gt; RL (GRPO: rule-based + consistency reward) -&gt; SFT (more diverse sft training data including both reasoning and none-reasoning) -&gt; RL (GRPO: rule-based + preference reward)</p>
<p>Most of the components are pretty clear demnstrated in below diagram, one thing to explicitly mention is the second stage SFT data - it utilize the first RL convergence checkpoint to collect high-quality reasoning and non-reasoning data through <strong>rejection sampling</strong>,</p>
<ul>
<li>Reasoning data: 600k, filtered out CoT with mixed languages, long paragraphs and code blocks</li>
<li>Non-reasoning data: 200k, a combination of subset of deepseek-v3 sft data plus CoT prompting data from deepseek-v3 sft(?) checkpoint, including writing, role-playing, factual QA, self-cognition and translation.</li>
</ul>
<p><img src="/../../image/r1/r1_workflow.png" alt="r1_workflow"></p>
<p>More experiments are not listed this post, feel free to refer original technique paper.</p>
<h2 id="Thoughts-and-Summarization"><a href="#Thoughts-and-Summarization" class="headerlink" title="Thoughts and Summarization"></a>Thoughts and Summarization</h2><p><strong>Data, Data, Data</strong></p>
<p><strong>High-quality training data</strong> at scale is fundamental to effective post-training. Many contemporary post-training techniques fundamentally aim to enhance data quality: distillation methods (both data and logit distillation) generate superior responses for learning, while rejection sampling fine-tunes models on their own high-quality outputs. Both approaches effectively enhance model performance.</p>
<p>Besides data quality, <strong>data diversity</strong> is also important. It helps to prevent overfiting and improve model generalization. It directly contributes the performance gap between in-distribution(ID) vs out-of-distribution(OOD) task performance. Good OOD performance indicates the model has learned generalizable patterns, not just memorized training data. Especially important in RL phase, one related work – <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.17161">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</a></p>
<h2 id="Disclaimer"><a href="#Disclaimer" class="headerlink" title="Disclaimer"></a>Disclaimer</h2><p>This post is intended solely for personal study and learning purposes. Feedback and corrections are welcome if you find any inaccuracies. The content does not represent the views of, nor disclose any research methods from, any company or research organization.</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Xin</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://example.com/2025/02/02/Deepseek-R1-and-GRPO/">http://example.com/2025/02/02/Deepseek-R1-and-GRPO/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2025/01/10/Regularization/">NN Regularization - Dropout In Additional Transformer Head</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Xin | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>